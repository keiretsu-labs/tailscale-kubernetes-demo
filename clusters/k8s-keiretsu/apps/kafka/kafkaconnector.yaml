apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnector
metadata:
  name: tesla-to-s3-connector
  namespace: kafka
  annotations:
    strimzi.io/use-connector-resources: "true"
  labels:
    strimzi.io/cluster: tesla-kafka-connect
spec:
  class: io.confluent.connect.s3.S3SinkConnector
  tasksMax: 1
  config:
    topics: "tesla_V" # Topic from Tesla Kafka cluster
    s3.bucket.name: "ceph-bkt-tesla" # Replace with your S3 bucket name
    s3.region: "us-east-1" # Replace with your AWS region
    store.url: "http://rook-ceph-rgw-my-store.rook-ceph"
    s3.part.size: "5242880" # 5 MB part size for large files
    s3.compression.type: "none" # No compression, since we need to store raw serialized data
    aws.access.key.id: ${file:/opt/kafka/connect/secrets/aws-credentials:AWS_ACCESS_KEY_ID} # AWS access key from the secret
    aws.secret.access.key: ${file:/opt/kafka/connect/secrets/aws-credentials:AWS_SECRET_ACCESS_KEY} # AWS secret access key from the secret
    format.class: "io.confluent.connect.s3.format.bytes.ByteArrayFormat" # Keep raw data without transformations
    schema.compatibility: "NONE" # No schema transformations
    path.format: "YYYY/MM/dd/HH"